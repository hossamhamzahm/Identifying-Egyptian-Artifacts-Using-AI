{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hossamhamza/resnet50v1-0-0?scriptVersionId=144433777\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#imports\nimport time\nimport torch\nimport torch.nn as nn #neural network module\nimport torch.optim as optim #optimizers (e.g. Gradient Descent Stochastic Gradient Descent (SGD))\nimport torch.nn.functional as F # contains activation functions\nfrom torch.utils.data import DataLoader # to load data set\nimport torchvision.datasets as datasets # to download dataset form mnisc\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-24T11:24:38.819092Z","iopub.execute_input":"2023-09-24T11:24:38.819519Z","iopub.status.idle":"2023-09-24T11:24:38.826019Z","shell.execute_reply.started":"2023-09-24T11:24:38.819485Z","shell.execute_reply":"2023-09-24T11:24:38.824873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timer(f, txt=\"time\"):\n    def wrapper(*args):\n        tic = time.time()\n        val = f(*args)\n        tac = time.time() - tic\n\n        print(txt, tac, \"ms\")\n        return val\n    return wrapper","metadata":{"execution":{"iopub.status.busy":"2023-09-24T11:24:38.832081Z","iopub.execute_input":"2023-09-24T11:24:38.833047Z","iopub.status.idle":"2023-09-24T11:24:38.84076Z","shell.execute_reply.started":"2023-09-24T11:24:38.832988Z","shell.execute_reply":"2023-09-24T11:24:38.839731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying Transforms to the Data\nimage_transforms = { \n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n        transforms.RandomRotation(degrees=15),\n        transforms.RandomHorizontalFlip(),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n}\n\n\n\n#Setup Gdrive file download extention \n# !pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-09-24T11:24:38.847688Z","iopub.execute_input":"2023-09-24T11:24:38.848798Z","iopub.status.idle":"2023-09-24T11:24:38.858695Z","shell.execute_reply.started":"2023-09-24T11:24:38.848753Z","shell.execute_reply":"2023-09-24T11:24:38.857644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set device \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# import gdown \n# url = 'https://drive.google.com/uc?id=1hBzNW2XOp41v58GPTDITJvN_xrug1U8b' \n# output = 'drive.zip'\n# gdown.download(url, output)\n\n# !conda install -y gdown \n# !gdown --id 1hBzNW2XOp41v58GPTDITJvN_xrug1U8b\n# ! unzip drive.zip\n# ! rm drive.zip\n\n# !python3 -m gdown --id 1hBzNW2XOp41v58GPTDITJvN_xrug1U8b\n\n\n\n# Set train and valid directory paths\n# train_directory = \"/kaggle/input/monuments-v0/monuments/monuments/train\"\n# test_directory = \"/kaggle/input/monuments-v0/monuments/monuments/test\"\n# data_directory = \"/kaggle/input/eg-landmarks/images\"\ndata_directory = \"/kaggle/input/kgl-dtst/kaggle\"\n\n# valid_directory = \"/kaggle/input/monuments-v0/monuments/monuments/valid\"\n\n# Batch size\nbs = 32\n\n\n# how many times the entire dataset is fed to the network\nepochs = 8\n\n\n# Load Data from folders\ndata = {\n    'train': datasets.ImageFolder(root=data_directory, transform=image_transforms['train']),\n    'valid': True,\n    'test': True\n}\n\n# Size of Data, to be used for calculating Average Loss and Accuracy\n# test_data2 = DataLoader(datasets.ImageFolder(root=test2_directory, transform=image_transforms['test']), batch_size=bs, shuffle=True)\n# test_data_size2 = len(datasets.ImageFolder(root=test2_directory, transform=image_transforms['test']))\nclass_names = data['train'].classes\n\n# Number of classes\nnum_classes = len(class_names) #5\n\n# train_data_size = len(data['train'])\n# valid_data_size = len(data['valid'])\n# test_data_size = len(data['test'])\n\n\n# Create iterators for the Data loaded using DataLoader module\ndata['train'], data['test'], data['valid'] = torch.utils.data.random_split(data['train'], [0.7, 0.15, 0.15], generator=torch.Generator().manual_seed(42))\ntrain_data_size = len(data['train'])\nvalid_data_size = len(data['test'])\ntest_data_size = len(data['valid'])\n\ntrain_data = DataLoader(data['train'], batch_size=bs, shuffle=True)\nvalid_data = DataLoader(data['valid'], batch_size=bs, shuffle=True)\ntest_data = DataLoader(data['test'], batch_size=bs, shuffle=True)\n\n# Print the train, validation and test set data sizes\ntrain_data_size, valid_data_size, test_data_size","metadata":{"execution":{"iopub.status.busy":"2023-09-24T11:24:38.879724Z","iopub.execute_input":"2023-09-24T11:24:38.880393Z","iopub.status.idle":"2023-09-24T11:24:38.942857Z","shell.execute_reply.started":"2023-09-24T11:24:38.880352Z","shell.execute_reply":"2023-09-24T11:24:38.941637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pretrained ResNet50 Model\nresnet50 = models.resnet50(pretrained=True) # True to allow learning transfer\n\n\n# Freeze model parameters\nfor param in resnet50.parameters():\n    param.requires_grad = False\n\n\n# Change the final layer of ResNet50 Model for Transfer Learning\nfc_inputs = resnet50.fc.in_features\nresnet50.fc = nn.Sequential(\n    nn.Linear(fc_inputs, 256),\n    nn.ReLU(),\n    nn.Dropout(0.4),\n    nn.Linear(256, num_classes), \n    nn.LogSoftmax(dim=1) # For using NLLLoss()\n)\n\n\n# Convert model to be used on GPU\nresnet50 = resnet50.to(device)\n\n\n# Define Optimizer and Loss Function\nloss_func = nn.NLLLoss()\noptimizer = optim.Adam(resnet50.parameters())\nhistory = []\n\n\n\n\ndef train():\n    for epoch in range(epochs):\n            epoch_start = time.time()\n            print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n            # Set to training mode\n            resnet50.train()\n            # Loss and Accuracy within the epoch\n            train_loss = 0.0\n            train_acc = 0.0\n            valid_loss = 0.0\n            valid_acc = 0.0\n            for i, (inputs, labels) in enumerate(train_data):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                # Clean existing gradients\n                optimizer.zero_grad()\n                # Forward pass - compute outputs on input data using the model\n                outputs = resnet50(inputs)\n                # Compute loss\n                loss = loss_func(outputs, labels)\n                # Backpropagate the gradients\n                loss.backward()\n                # Update the parameters\n                optimizer.step()\n                # Compute the total loss for the batch and add it to train_loss\n                train_loss += loss.item() * inputs.size(0)\n                # Compute the accuracy\n                ret, predictions = torch.max(outputs.data, 1)\n                correct_counts = predictions.eq(labels.data.view_as(predictions))\n                # Convert correct_counts to float and then compute the mean\n                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n                # Compute total accuracy in the whole batch and add to train_acc\n                train_acc += acc.item() * inputs.size(0)\n                print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n    # Validation - No gradient tracking needed\n            with torch.no_grad():\n                # Set to evaluation mode\n                resnet50.eval()\n                # Validation loop\n                for j, (inputs, labels) in enumerate(valid_data):\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n                    # Forward pass - compute outputs on input data using the model\n                    outputs = resnet50(inputs)\n                    # Compute loss\n                    loss = loss_func(outputs, labels)\n                    # Compute the total loss for the batch and add it to valid_loss\n                    valid_loss += loss.item() * inputs.size(0)\n                    # Calculate validation accuracy\n                    ret, predictions = torch.max(outputs.data, 1)\n                    correct_counts = predictions.eq(labels.data.view_as(predictions))\n                    # Convert correct_counts to float and then compute the mean\n                    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n                    # Compute total accuracy in the whole batch and add to valid_acc\n                    valid_acc += acc.item() * inputs.size(0)\n                    print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n            # Find average training loss and training accuracy\n            avg_train_loss = train_loss/train_data_size \n            avg_train_acc = train_acc/float(train_data_size)\n            # Find average training loss and training accuracy\n            avg_valid_loss = valid_loss/valid_data_size \n            avg_valid_acc = valid_acc/float(valid_data_size)\n            history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n            epoch_end = time.time()\n            print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, nttValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T11:24:38.945429Z","iopub.execute_input":"2023-09-24T11:24:38.945899Z","iopub.status.idle":"2023-09-24T11:24:39.505078Z","shell.execute_reply.started":"2023-09-24T11:24:38.945862Z","shell.execute_reply":"2023-09-24T11:24:39.50392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chech_accuracy(loader, model):\n    num_correct=0\n    num_samples=0\n    model.eval()\n\n#  This method disables the gradient calculation which reduces the memory consumption for computations.\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            \n            \n            num_correct += (y == predictions).sum()\n            num_samples += predictions.size(0)\n\n        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)}')\n    \n\n\n\ndef predict(model, test_image_name):\n    transform = image_transforms['test']\n    test_image = Image.open(test_image_name)\n    plt.imshow(test_image)\n    test_image_tensor = transform(test_image)\n    if torch.cuda.is_available():\n        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n    else:\n        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n    with torch.no_grad():\n        model.eval()\n        # Model outputs log probabilities\n        out = model(test_image_tensor)\n        ps = torch.exp(out)\n        topk, topclass = ps.topk(1, dim=1)\n        print(\"Output class :  \", idx_to_class[topclass.cpu().numpy()[0][0]])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T11:24:39.50718Z","iopub.execute_input":"2023-09-24T11:24:39.50782Z","iopub.status.idle":"2023-09-24T11:24:39.521454Z","shell.execute_reply.started":"2023-09-24T11:24:39.507783Z","shell.execute_reply":"2023-09-24T11:24:39.520331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_path = \"/kaggle/working/params\"\n\n\ntrain()\ntorch.save(resnet50.state_dict(), params_path)\n# resnet50.load_state_dict(torch.load(params_path))\n\nprint(class_names)\nchech_accuracy(test_data, resnet50)\n# chech_accuracy(valid_data, resnet50)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T11:24:39.523677Z","iopub.execute_input":"2023-09-24T11:24:39.524348Z","iopub.status.idle":"2023-09-24T11:27:18.092328Z","shell.execute_reply.started":"2023-09-24T11:24:39.524309Z","shell.execute_reply":"2023-09-24T11:27:18.089769Z"},"trusted":true},"execution_count":null,"outputs":[]}]}